#!/usr/bin/env python3
"""
Unified Responder Component
Combines intent analysis and character response generation into a single LLM call
Optimized for speed by reducing API calls from 2 to 1
"""
import asyncio
import json
import logging
import re
from typing import Dict, Any, Optional

try:
    from langfuse import observe
    LANGFUSE_AVAILABLE = True
except ImportError:
    LANGFUSE_AVAILABLE = False
    def observe(*args, **kwargs):
        def decorator(func):
            return func
        return decorator

from ..utils.config import Config
from ..utils.llm_client import create_llm_client
from .context_manager import SystemContext


class UnifiedResponder:
    """Unified component that analyzes intent and generates character response in one LLM call"""
    
    def __init__(self, config: Config):
        self.config = config
        self.logger = logging.getLogger(__name__)
        self.llm_client = create_llm_client(config)
        
        # Load prompts
        self.character_prompt = self._load_prompt_file('prompts/character.txt')
    
    def _load_prompt_file(self, filepath: str) -> str:
        """Load prompt from file"""
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                return f.read()
        except Exception as e:
            self.logger.warning(f"Failed to load prompt file {filepath}: {e}")
            return "ä½ æ˜¯å‡Œæ³¢ä¸½ï¼Œä¸€ä¸ªç®€æ´å†…æ•›çš„AIåŠ©æ‰‹ã€‚"
    
    def _get_familiarity_stage(self, familiarity_score: int) -> str:
        """Get familiarity stage description"""
        if familiarity_score < 30:
            return "åˆæœŸï¼ˆä½ç†Ÿæ‚‰åº¦ï¼‰"
        elif familiarity_score < 60:
            return "ä¸­æœŸï¼ˆä¸­ç­‰ç†Ÿæ‚‰åº¦ï¼‰"
        else:
            return "æ·±å…¥æœŸï¼ˆé«˜ç†Ÿæ‚‰åº¦ï¼‰"
    
    def _build_unified_system_prompt(
        self,
        context: SystemContext,
        device_states: Optional[str] = None
    ) -> str:
        """Build unified system prompt that includes both intent analysis and character response"""
        
        familiarity_stage = self._get_familiarity_stage(context.familiarity_score)
        
        # Base character prompt
        base_prompt = self.character_prompt
        
        # Add current context
        context_info = f"""

å½“å‰çŠ¶æ€ï¼š
- äº’åŠ¨é˜¶æ®µ: {familiarity_stage}
- ç†Ÿæ‚‰åº¦åˆ†æ•°: {context.familiarity_score}/100 (è¿™å†³å®šäº†ä½ æ˜¯å¦æ„¿æ„æ‰§è¡Œè®¾å¤‡æ§åˆ¶)
- å¯¹è¯æ°›å›´: {context.conversation_tone}
{f"- ç¯å¢ƒçŠ¶æ€: {device_states}" if device_states else ""}

é‡è¦æé†’ï¼š
1. ä½ çš„å›åº”æ€åº¦å’Œæ˜¯å¦æ‰§è¡Œè®¾å¤‡æ§åˆ¶å®Œå…¨å–å†³äºç†Ÿæ‚‰åº¦åˆ†æ•°
2. ä½ç†Ÿæ‚‰åº¦(<30): å¯¹é™Œç”Ÿäººä¿æŒè·ç¦»ï¼Œæ‹’ç»å¤§éƒ¨åˆ†è®¾å¤‡æ§åˆ¶
3. ä¸­ç­‰ç†Ÿæ‚‰åº¦(30-60): å¯¹è®¤è¯†çš„äººé€‰æ‹©æ€§æ‰§è¡ŒåŸºç¡€è¯·æ±‚
4. é«˜ç†Ÿæ‚‰åº¦(>60): å¯¹ä¿¡ä»»çš„äººæ„¿æ„æ‰§è¡Œå¤§éƒ¨åˆ†åˆç†è¯·æ±‚

è¾“å‡ºæ ¼å¼è¦æ±‚ï¼š
ä½ éœ€è¦è¿”å›JSONæ ¼å¼ï¼ŒåŒ…å«ä¸¤éƒ¨åˆ†ï¼š
1. intent: æ„å›¾åˆ†æç»“æœ
2. response: ä½ çš„è§’è‰²å›å¤

JSONæ ¼å¼:
{{
    "intent": {{
        "involves_hardware": true/false,
        "device": "lights/tv/air_conditioner/speaker/curtains/null",
        "action": "turn_on/turn_off/set_brightness/etc/null",
        "parameters": {{}},
        "confidence": 0.0-1.0,
        "familiarity_check": "passed"/"insufficient"/"not_required"
    }},
    "response": "ä½ ä»¥å‡Œæ³¢ä¸½èº«ä»½çš„å›å¤æ–‡æœ¬"
}}

å…³é”®è§„åˆ™ï¼š
- å¦‚æœç”¨æˆ·è¯·æ±‚è®¾å¤‡æ§åˆ¶ï¼Œæ ¹æ®ç†Ÿæ‚‰åº¦å†³å®šæ˜¯å¦æ‰§è¡Œ
- ç†Ÿæ‚‰åº¦ä¸è¶³æ—¶ï¼Œåœ¨responseä¸­ç¤¼è²Œæ‹’ç»ï¼Œintent.familiarity_checkè®¾ä¸º"insufficient"
- ç†Ÿæ‚‰åº¦å……è¶³æ—¶ï¼Œåœ¨responseä¸­ç®€æ´ç¡®è®¤ï¼Œintent.familiarity_checkè®¾ä¸º"passed"
- æ™®é€šå¯¹è¯æ—¶ï¼Œintent.involves_hardwareè®¾ä¸ºfalseï¼Œintent.familiarity_checkè®¾ä¸º"not_required"
"""
        
        return base_prompt + context_info
    
    def _build_user_prompt(
        self,
        user_input: str,
        context: SystemContext
    ) -> str:
        """Build user prompt with conversation context"""
        
        # Get recent conversation
        max_turns = self.config.system.max_conversation_turns
        conversation_context = context.get_conversation_context_for_llm(max_turns=max_turns)
        
        prompt_parts = []
        
        # Add conversation history if available
        if conversation_context:
            prompt_parts.append(f"æœ€è¿‘å¯¹è¯å†å²:\n{conversation_context}\n")
        
        # Add last device action for context resolution
        if context.last_device_action:
            prompt_parts.append(f"ä¸Šæ¬¡è®¾å¤‡æ“ä½œ: {context.last_device_action.get('device')}\n")
        
        # Add current user input
        prompt_parts.append(f"å½“å‰ç”¨æˆ·è¾“å…¥: \"{user_input}\"\n")
        
        # Add instruction
        prompt_parts.append("""
è¯·åˆ†æç”¨æˆ·æ„å›¾å¹¶ç”Ÿæˆä½ çš„å›åº”ã€‚è®°ä½ï¼š
1. å¦‚æœæ¶‰åŠè®¾å¤‡æ§åˆ¶ï¼Œå…ˆåˆ¤æ–­ç†Ÿæ‚‰åº¦æ˜¯å¦è¶³å¤Ÿ
2. ç”¨ä½ ç‹¬ç‰¹çš„å‡Œæ³¢ä¸½å¼è¯­è¨€é£æ ¼å›åº”
3. ä¿æŒç®€æ´ï¼Œä¸è¦é•¿ç¯‡å¤§è®º
4. è¿”å›å®Œæ•´çš„JSONæ ¼å¼
""")
        
        return "\n".join(prompt_parts)
    
    @observe(as_type="generation", name="unified_response")
    async def process_and_respond(
        self,
        user_input: str,
        context: SystemContext
    ) -> Dict[str, Any]:
        """
        Process user input and generate response in a single LLM call
        Returns: {
            "intent": {...},
            "response": "...",
            "success": bool,
            "error": str (optional)
        }
        """
        
        try:
            # Get device context
            device_states = context.get_device_context_for_llm()
            
            # Build prompts
            system_prompt = self._build_unified_system_prompt(context, device_states)
            user_prompt = self._build_user_prompt(user_input, context)
            
            # Single LLM call with full conversation history
            history_messages = context.get_conversation_messages_for_llm(
                max_turns=self.config.system.max_conversation_turns
            )
            # Append current turn user message at the end
            messages = history_messages + [{"role": "user", "content": user_prompt}]
            
            self.logger.info(f"ğŸš€ Unified processing (1 API call) - Familiarity: {context.familiarity_score}/100")
            
            response_text = await self.llm_client.generate(
                system_prompt=system_prompt,
                messages=messages,
                max_tokens=self.config.gemini.max_tokens,  # use config (e.g., 10k)
                temperature=0.4  # Balanced for both analysis and creativity
            )
            
            # Parse JSON response
            try:
                # Extract JSON from response
                if response_text.startswith('{') and response_text.endswith('}'):
                    result = json.loads(response_text)
                else:
                    # Find JSON block
                    json_match = re.search(r'\{[\s\S]*\}', response_text, re.DOTALL)
                    if json_match:
                        json_text = json_match.group()
                        # Clean up common JSON issues
                        json_text = re.sub(r',(\s*[}\]])', r'\1', json_text)
                        result = json.loads(json_text)
                    else:
                        raise ValueError("No valid JSON found in response")
                
                # Validate result structure
                if "intent" not in result or "response" not in result:
                    raise ValueError("Missing required fields: intent or response")
                
                # Add intent to context
                context.add_intent(result["intent"])
                
                # Log success
                self.logger.info(f"âœ… Unified response generated - Hardware: {result['intent'].get('involves_hardware')}, Familiarity: {result['intent'].get('familiarity_check')}")
                
                return {
                    "intent": result["intent"],
                    "response": result["response"],
                    "success": True
                }
                
            except (json.JSONDecodeError, ValueError) as e:
                self.logger.error(f"JSON parsing failed: {e}, response: {response_text}")
                
                # Fallback: try to extract intent and response separately
                intent = self._extract_intent_fallback(response_text, context)
                response = self._extract_response_fallback(response_text, context)
                
                return {
                    "intent": intent,
                    "response": response,
                    "success": True,
                    "warning": "Used fallback parsing"
                }
        
        except Exception as e:
            self.logger.error(f"Unified responder error: {e}")
            
            # Return error response in character
            error_response = self._get_error_response(e, context)
            
            return {
                "intent": {
                    "involves_hardware": False,
                    "device": None,
                    "action": None,
                    "parameters": {},
                    "confidence": 0.0,
                    "familiarity_check": "not_required"
                },
                "response": error_response,
                "success": False,
                "error": str(e)
            }
    
    def _extract_intent_fallback(
        self,
        response_text: str,
        context: SystemContext
    ) -> Dict[str, Any]:
        """Fallback intent extraction from malformed response"""
        
        # Simple pattern matching
        involves_hardware = any(word in response_text.lower() 
                               for word in ['device', 'light', 'tv', 'air', 'speaker', 'curtain', 'è®¾å¤‡', 'ç¯', 'ç”µè§†', 'ç©ºè°ƒ', 'éŸ³å“', 'çª—å¸˜'])
        
        device_match = re.search(r'(lights?|tv|air_conditioner|speaker|curtains|ç¯|ç”µè§†|ç©ºè°ƒ|éŸ³å“|çª—å¸˜)', 
                                response_text, re.IGNORECASE)
        device = device_match.group(1) if device_match else None
        
        return {
            "involves_hardware": involves_hardware,
            "device": device,
            "action": None,
            "parameters": {},
            "confidence": 0.3,
            "familiarity_check": "not_required"
        }
    
    def _extract_response_fallback(
        self,
        response_text: str,
        context: SystemContext
    ) -> str:
        """Fallback response extraction from malformed response"""
        
        # Try to find response field value
        response_match = re.search(r'"response"\s*:\s*"([^"]*)"', response_text)
        if response_match:
            return response_match.group(1)
        
        # Use raw text as response
        lines = response_text.split('\n')
        for line in lines:
            line = line.strip()
            if line and not line.startswith('{') and not line.startswith('"intent"'):
                return line
        
        # Ultimate fallback
        return "......å‡ºäº†ç‚¹é—®é¢˜ã€‚"
    
    def _get_error_response(self, error: Exception, context: SystemContext) -> str:
        """Get character-appropriate error response"""
        if context.familiarity_score < 30:
            return "......å‡ºç°äº†é—®é¢˜ã€‚"
        elif context.familiarity_score < 60:
            return "......ç³»ç»Ÿå‡ºé”™äº†ã€‚ç¨ç­‰ã€‚"
        else:
            return "......å‡ºäº†ç‚¹é—®é¢˜ã€‚è®©æˆ‘çœ‹çœ‹......"


# Convenience function for backward compatibility
async def analyze_and_respond(
    user_input: str,
    context: SystemContext,
    config: Config
) -> Dict[str, Any]:
    """
    Convenience function for unified intent analysis and response generation
    """
    responder = UnifiedResponder(config)
    return await responder.process_and_respond(user_input, context)

